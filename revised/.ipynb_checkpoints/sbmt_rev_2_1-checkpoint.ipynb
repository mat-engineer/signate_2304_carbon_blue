{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a0877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e12b79",
   "metadata": {},
   "source": [
    "# 1 データ読み込み\n",
    "- trainデータとtestデータ共に読み込んでマージ、不要な情報列を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b3071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv('train_data.csv',index_col=0)\n",
    "df_test = pd.read_csv(\"test_data.csv\",index_col=0)\n",
    "df_test['cover'] = 'a'\n",
    "df = pd.concat([df_tr,df_test])\n",
    "df.drop(columns=['Landsat_StartTime','YMD','PRODUCT_ID'],inplace=True)\n",
    "df['year'] = df['year'].astype(int)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d19c5a",
   "metadata": {},
   "source": [
    "# 2 データ前処理\n",
    "## 2-1 クラスタリング\n",
    "### 2-1-1 ラベルエンコーディング\n",
    "- 'mesh20'をラベルエンコーディング実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea2e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()    \n",
    "le.fit(df[['mesh20']])\n",
    "list_label = sorted(list(set(le.classes_)))\n",
    "map_label = {j:i for i,j in enumerate(list_label)}\n",
    "dict_mesh = {}\n",
    "dict_mesh['map_label'] = map_label\n",
    "map_label = dict_mesh['map_label']\n",
    "df['mesh20'] = df['mesh20'].map(map_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a41ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df[df['cover']!='a']\n",
    "df_ts = df[df['cover'] == 'a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8f7f6",
   "metadata": {},
   "source": [
    "### 2-1-2 地理的に３つの区域（３クラス）にクラスタリング\n",
    "- 経度、緯度情報のみを使用して３クラスに分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4eb3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clst = KMeans(n_clusters=3,\n",
    "            init='k-means++',\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            random_state=0)\n",
    "pred = clst.fit_predict(df_tr[['lat','lon']])\n",
    "df_tr['cluster_id'] = pred\n",
    "pred_ts = clst.predict(df_ts[['lat', 'lon']])\n",
    "df_ts['cluster_id'] = pred_ts\n",
    "df = pd.concat([df_tr,df_ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767cda6",
   "metadata": {},
   "source": [
    "### 2-1-3 サブクラス作成\n",
    "- 3つの区域（クラス）毎にサブクラスを３クラス作成_今回は全データを使ってクラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "401ced98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df[df['cover'] != 'a']\n",
    "df_ts = df[df['cover'] == 'a']\n",
    "df_ts = df_ts.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10faa4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cls_tr = df_tr.copy()\n",
    "df_cls_tr.drop(columns=['cover','cluster_id'],inplace=True)\n",
    "df_cls_tr = df_cls_tr.fillna(df_cls_tr.median())\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_cls_tr)\n",
    "df_cls_tr = pd.DataFrame(scaler.transform(df_cls_tr),columns=df_cls_tr.columns)\n",
    "df_cls_tr['cluster_id'] = df_tr['cluster_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f8a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cls_ts = df_ts.copy()\n",
    "df_cls_ts.drop(columns=['cover','cluster_id'],inplace=True)\n",
    "df_cls_ts = df_cls_ts.fillna(df_cls_ts.median())\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_cls_ts)\n",
    "df_cls_ts = pd.DataFrame(scaler.transform(df_cls_ts),columns=df_cls_ts.columns)\n",
    "df_cls_ts['cluster_id'] = df_ts['cluster_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36052cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clst = KMeans(n_clusters=3,\n",
    "            init='k-means++',\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            random_state=0)\n",
    "tmp_df_tr = pd.DataFrame()\n",
    "for i in range(3):\n",
    "    pred = clst.fit_predict(df_cls_tr[df_cls_tr['cluster_id'] == i])\n",
    "    filename = 'kmeans_model.pkl'\n",
    "    pickle.dump(clst, open(filename, 'wb'))\n",
    "    tmp_tr = pd.DataFrame({'id': df_cls_tr[df_cls_tr['cluster_id'] == i].index, \n",
    "                        'cls_id_2': pred})\n",
    "    \n",
    "    tmp_df_tr = pd.concat([tmp_df_tr,tmp_tr])\n",
    "    \n",
    "tmp_tr = tmp_df_tr.sort_values('id').reset_index(drop=True)\n",
    "df_tr['cls_id_2'] = tmp_tr['cls_id_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f13ea42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_df_ts = pd.DataFrame()\n",
    "filename = 'kmeans_model.pkl'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "for i in range(3):\n",
    "    subset = df_cls_ts[df_cls_ts['cluster_id'] == i]\n",
    "    pred_ts = loaded_model.predict(subset)\n",
    "    tmp_ts = pd.DataFrame({'id': df_cls_ts[df_cls_ts['cluster_id'] == i].index, \n",
    "                        'cls_id_2': pred_ts})\n",
    "    tmp_df_ts = pd.concat([tmp_df_ts,tmp_ts])\n",
    "\n",
    "tmp_ts = tmp_df_ts.sort_values('id').reset_index(drop=True)\n",
    "df_ts['cls_id_2'] = tmp_ts['cls_id_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd5ce056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_tr,df_ts])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b6f14",
   "metadata": {},
   "source": [
    "## 2-2 欠損値補完\n",
    "- 主に画像データの欠損値を補完する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb16a5",
   "metadata": {},
   "source": [
    "### 2-2-1 欠損値行の特定\n",
    "   - 画像データのうち、50％以上が欠損値の行を補完する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e428e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行内のNaNの割合を計算する\n",
    "nan_rows = df.iloc[:,85:3460].isna().sum(axis=1) / df.iloc[:,85:3460].shape[1]\n",
    "# NaNの割合が50%以上の行を抽出する\n",
    "high_nan_rows = df.iloc[:,85:3460][nan_rows >= 0.5]\n",
    "nan_row_list = high_nan_rows.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17ef04",
   "metadata": {},
   "source": [
    "### 2-2-2 欠損値補完A\n",
    "- 年毎のランドサットデータは前後1年のデータで補完する、2000年時は2001、2002のデータで補完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a34f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_column_values(arr, row, cols):\n",
    "    for col in cols:\n",
    "        if not np.isnan(arr[row, col]):\n",
    "            return arr[row, col]\n",
    "    return np.nan\n",
    "arr = df.iloc[:,310:3460].to_numpy(dtype=float)\n",
    "# 各列の欠損値を指定された条件で埋める\n",
    "for j in range(3150):\n",
    "    missing_rows = np.where(np.isnan(arr[:, j]))[0]\n",
    "    for i in missing_rows:\n",
    "        if j >= 0 and j < 150:\n",
    "            cols = [j + 150, j + 300]\n",
    "            cols = [col for col in cols if col >= 0 and col < arr.shape[1]]\n",
    "            arr[i, j] = fillna_with_column_values(arr, i, cols)\n",
    "        else:\n",
    "            cols = [j - 150, j + 150]\n",
    "            cols = [col for col in cols if col >= 0 and col < arr.shape[1]]\n",
    "            arr[i, j] = fillna_with_column_values(arr, i, cols)\n",
    "        \n",
    "df.iloc[:,310:3460] = pd.DataFrame(arr, columns=df.iloc[:,310:3460].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97aae43",
   "metadata": {},
   "source": [
    "### 2-2-3 欠損値補完B\n",
    "- Aで補完できなかった行を補完する\n",
    "- 最寄りの５箇所を特定して、５箇所の平均値で補完する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3fd679",
   "metadata": {},
   "source": [
    "#### 2-2-3-1 最寄りの５箇所を抽出\n",
    "   - 緯度経度からターゲットから最も近い５箇所を抽出する（trainデータで補完する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df[df['cover'] != 'a']\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)\n",
    "    lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def closest_n_locations(df, index, n, exclude_indices):\n",
    "    target_lat, target_lon = df.loc[index, 'lat'], df.loc[index, 'lon']\n",
    "    distances = df.apply(lambda row: haversine(\n",
    "        target_lat, target_lon, row['lat'], row['lon']), axis=1)\n",
    "    distances = distances.drop(exclude_indices)\n",
    "    closest_indices = distances.nsmallest(n+1).iloc[1:].index\n",
    "    closest_rows = df.loc[closest_indices].copy()\n",
    "    closest_rows['元の行番号'] = index  \n",
    "    closest_rows['距離'] = distances[closest_indices].values  # 距離を追加\n",
    "    return closest_rows\n",
    "\n",
    "def closest_n_locations_for_indices(df, indices, n):\n",
    "    result_df = pd.DataFrame()\n",
    "    for index in indices:\n",
    "        closest_n = closest_n_locations(df, index, n, indices)\n",
    "        result_df = pd.concat([result_df, closest_n])\n",
    "    return result_df.reset_index().rename(columns={'index': '抽出された行番号'})\n",
    "\n",
    "# リスト内の各行番号に対して最も近い場所5箇所を抽出\n",
    "closest_5_for_indices = closest_n_locations_for_indices(\n",
    "    df_tr[['lat','lon']], nan_row_list, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507aea12",
   "metadata": {},
   "source": [
    "#### 2-2-3-2 ５箇所の平均値で欠損値を補完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209de2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = closest_5_for_indices[['元の行番号','抽出された行番号']]\n",
    "for original_index in tmp['元の行番号'].unique():\n",
    "    closest_rows = tmp.loc[tmp['元の行番号'] == original_index, '抽出された行番号']\n",
    "    mean_values = df.iloc[closest_rows,310:3460].mean()\n",
    "    df.iloc[original_index,310:3460] = df.iloc[original_index,310:3460].fillna(\n",
    "        mean_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f2651",
   "metadata": {},
   "source": [
    "### 2-2-4　欠損値補間C\n",
    "- 時系列ランドサットデータの欠損値を補完する（測定年と同年の年ごとのランドセット画像MEDで補完する）\n",
    "#### 2-2-4-1 補完対応表の作成\n",
    "   - 時系列ランドサットデータと年毎のランドサット画像MEDの補完する際の列対応表を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pd.read_csv('df_table_1.csv').reset_index(drop=True)\n",
    "table_dict = {}\n",
    "for index, row in df_table.iterrows():\n",
    "    key = int(row['A'])\n",
    "    value = row['B']    \n",
    "    if pd.notna(value) and float(value).is_integer():\n",
    "        value = int(value)\n",
    "    elif pd.isna(value):\n",
    "        value = float('nan')\n",
    "    table_dict[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bce0ab",
   "metadata": {},
   "source": [
    "#### 2-2-4-2 欠損値補完（対応表に従って）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M列の値に応じて欠損値を埋める関数\n",
    "def fillna_based_on_m(row):\n",
    "    m = row[\"year\"]\n",
    "    # M列の値が0から20の範囲内である場合のみ、欠損値を埋める処理を実行\n",
    "    if 1999 <= m <= 2020:\n",
    "        if m != 1999:\n",
    "            start_col = 24\n",
    "            end_col = 84\n",
    "            offset = 150 * (m - 2000)\n",
    "\n",
    "            for i in range(start_col, end_col):\n",
    "                if np.isnan(row[i]):\n",
    "                    if i in table_dict:\n",
    "                        # 対応する列を計算し、値をコピー\n",
    "                        source_col = table_dict[i-23] + 359 + offset\n",
    "                        if not math.isnan(source_col):\n",
    "                            row[i] = row[source_col]\n",
    "                        else:\n",
    "                            row[i] = float(\"nan\")\n",
    "                    else:\n",
    "                        # 対応する列がない場合はNanに\n",
    "                        row[i] = float(\"nan\")\n",
    "        else:\n",
    "            start_col = 24\n",
    "            end_col = 84\n",
    "            offset = 0\n",
    "\n",
    "            for i in range(start_col, end_col):\n",
    "                if np.isnan(row[i]):\n",
    "                    if i in table_dict:\n",
    "                        # 対応する列を計算し、値をコピー\n",
    "                        source_col = table_dict[i-23] + 359 + offset\n",
    "                        if not math.isnan(source_col):\n",
    "                            row[i] = row[source_col]\n",
    "                        else:\n",
    "                            row[i] = float(\"nan\")\n",
    "                    else:\n",
    "                        # 対応する列がない場合はNanに\n",
    "                        row[i] = float(\"nan\")\n",
    "\n",
    "    return row\n",
    "# dfにfillna_based_on_m関数を適用\n",
    "df = df.apply(fillna_based_on_m, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec69588",
   "metadata": {},
   "source": [
    "## 2-3 特徴量作成\n",
    "### 2-3-1 最寄りの'cover'を特徴量に追加\n",
    "- 予測箇所から最も近い場所１０箇所を抽出して、予測箇所からの距離毎に特徴量に追加する\n",
    "- 特徴量追加は、trainデータには、traiｎデータから、testデータにはtrain+testデータから追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ffd60",
   "metadata": {},
   "source": [
    "#### 2-3-1-1 traiｎデータとtestデータに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a29520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df[df['cover'] != 'a']\n",
    "df_ts = df[df['cover'] == 'a']\n",
    "df_tr_list = df_tr.index\n",
    "df_ts_list = df_ts.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebbdac",
   "metadata": {},
   "source": [
    "#### 2-3-1-2 最寄りの１０箇所を抽出する\n",
    "- trainデータ用の１０箇所とtestデータ用の１０箇所それぞれ抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_10(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)\n",
    "    lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def closest_10_locations(df, index, n, exclude_indices):\n",
    "    target_lat, target_lon = df.loc[index, 'lat'], df.loc[index, 'lon']\n",
    "    distances = df.apply(lambda row: haversine_10(\n",
    "        target_lat, target_lon, row['lat'], row['lon']), axis=1)\n",
    "    distances = distances.drop(exclude_indices)\n",
    "    closest_indices = distances.nsmallest(n+1).iloc[1:].index\n",
    "    closest_rows = df.loc[closest_indices].copy()\n",
    "    closest_rows['元の行番号'] = index  \n",
    "    closest_rows['距離'] = distances[closest_indices].values  # 距離を追加\n",
    "    return closest_rows\n",
    "\n",
    "def closest_10_locations_for_indices(df, indices,exclude_indices, n):\n",
    "    result_df = pd.DataFrame()\n",
    "    for index in indices:\n",
    "        closest_n = closest_n_locations(df, index, n, exclude_indices)\n",
    "        result_df = pd.concat([result_df, closest_n])\n",
    "    return result_df.reset_index().rename(columns={'index': '抽出された行番号'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24476e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト内の各行番号に対して最も近い場所5箇所を抽出\n",
    "closest_10_for_test = closest_10_locations_for_indices(\n",
    "    df[['lat','lon']], df_ts_list, df_ts_list, 10)\n",
    "closest_10_for_train = closest_10_locations_for_indices(\n",
    "    df[['lat','lon']], df_tr_list, df_ts_list, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f588234",
   "metadata": {},
   "source": [
    "#### 2-3-1-3 抽出した１０箇所を距離毎に分類して特徴量に加える\n",
    "- trainデータ用testデータ用をマージして'cover'の値を取り出せるように加工\n",
    "- ターゲットからの距離が100m未満は’cover_0'列へ100-200m以内は’cover_1'列へ、、、1km以上は'cover_10'列に追加するようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp_1 = pd.concat([closest_10_for_train,closest_10_for_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ce817",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_tmp_1 = df_tmp_1.sort_values(['元の行番号','距離']).groupby(\n",
    "    '元の行番号')['抽出された行番号'].apply(list).reset_index()\n",
    "grouped_tmp_2 = df_tmp_1.sort_values(['元の行番号','距離']).groupby(\n",
    "    '元の行番号')['距離'].apply(list).reset_index()\n",
    "grouped = pd.concat([grouped_tmp_1,grouped_tmp_2['距離']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp_3を作成\n",
    "df_tmp_3 = pd.DataFrame()\n",
    "for i, row in grouped.iterrows():\n",
    "    extracted_row_numbers = row['抽出された行番号']\n",
    "    distances = row['距離']\n",
    "    \n",
    "    distance_cover_dict = dict(zip(distances, extracted_row_numbers))\n",
    "    \n",
    "    for j in range(11):\n",
    "        lower_bound = j * 0.1\n",
    "        \n",
    "        if j < 10:\n",
    "            upper_bound = (j + 1) * 0.1\n",
    "            cover_indices = [v for k, v in distance_cover_dict.items() if lower_bound <= k < upper_bound]\n",
    "        else:\n",
    "            cover_indices = [v for k, v in distance_cover_dict.items() if lower_bound <= k]\n",
    "        \n",
    "        if cover_indices:\n",
    "            covers = df.loc[cover_indices, 'cover'].values\n",
    "            df_tmp_3.loc[row['元の行番号'], f'cover_{j}'] = np.mean(covers, dtype=np.float64) \n",
    "        else:\n",
    "            df_tmp_3.loc[row['元の行番号'], f'cover_{j}'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0303b26",
   "metadata": {},
   "source": [
    "#### 2-3-1-4 上記の特徴量の統計値を特徴量に加える\n",
    "- 平均、標準偏差、最大、最小、max-minを加える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a042d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp_3['mean'] = df_tmp_3.mean(axis=1)\n",
    "df_tmp_3['max'] = df_tmp_3.max(axis=1)\n",
    "df_tmp_3['min'] = df_tmp_3.min(axis=1)\n",
    "df_tmp_3['max_min'] = df_tmp_3['max'] - df_tmp_3['min']\n",
    "df_tmp_3['std'] = df_tmp_3.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9441ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_tmp_3],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa7918",
   "metadata": {},
   "source": [
    "### 2-3-2 外れ値対応\n",
    "- ４分位をとって75%-25%の10倍以上離れた値をNanに置き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fca09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(cl_df):\n",
    "    for column in cl_df.columns:\n",
    "        # 列が数値データの場合のみ四分位範囲を計算\n",
    "        if np.issubdtype(cl_df[column].dtypes, np.number):\n",
    "            Q1 = cl_df[column].quantile(0.25)\n",
    "            Q3 = cl_df[column].quantile(0.75)\n",
    "            median = cl_df[column].median()\n",
    "            IQR = Q3 - Q1 \n",
    "            if IQR < median/10 :\n",
    "                # 25%パーセンタイル値と75%パーセンタイル値が同じ場合、平均値の10倍以上を外れ値とする\n",
    "                cl_df[column] = cl_df[column].apply(\n",
    "                    lambda x: x if( median / 10 <= x <= median * 10 )else np.nan)                   \n",
    "            else:\n",
    "                # 外れ値の範囲を定義\n",
    "                lower_bound = Q1 - 10 * IQR\n",
    "                upper_bound = Q3 + 10 * IQR\n",
    "\n",
    "                # 範囲内の値に制限\n",
    "                cl_df[column] = cl_df[column].apply(\n",
    "                    lambda x: x if lower_bound <= x <= upper_bound else np.nan)\n",
    "    return cl_df\n",
    "# 外れ値を取り除いたDataFrameを作成\n",
    "df = remove_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531ddaa",
   "metadata": {},
   "source": [
    "### 2-3-3 海洋環境要因データを加工して特徴量に加える\n",
    "- ’cover'に影響があると推測される下記特徴量を追加する\n",
    "    - ’ef_cliff'を追加→'cliff_length'(海崖長)が０ではなく'coastal_dist'(海岸までの距離)が近いと影響があると仮説\n",
    "    - ’ef_art'を追加→'aicial_length'(人工海岸線長）は埋立で'beach_length'(海浜長)’coast_length'と比較して値が大きく、かつ海岸線までの距離が近いと影響があると仮説\n",
    "    - 'ef_bch'を追加→’biach_length'(海浜長）の比率が高く、かつ'coastal_dist'(海岸までの距離)が近いと影響があると仮説\n",
    "    - 'ef_river'を追加→’river_area'(集水面積)が大きく'river_dist'(河口までの距離）が近いと影響があると仮説"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c434927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['cliff_length'].isnull()) & \n",
    "       (~df['aicial_length'].isnull() | \n",
    "        ~df['beach_length'].isnull()), 'cliff_length'] = 1\n",
    "# df_tmp['ef_cliff'] = df['cliff_length']\n",
    "df['ef_cliff'] = df.apply(\n",
    "    lambda row: row['cliff_length'] * 2 \n",
    "    if row['coastal_dist'] == 0 else row['cliff_length'] / row['coastal_dist'], axis=1)\n",
    "\n",
    "df['ef_art'] = df['aicial_length'] \\\n",
    "/ (df['aicial_length'] + df['beach_length'])\n",
    "df.loc[(df['ef_art'].isnull()) & (\n",
    "    ~df['aicial_length'].isnull() | ~df['beach_length'].isnull()), 'ef_art'] = 0\n",
    "\n",
    "df['ef_art'] = df.apply(\n",
    "    lambda row: row['ef_art'] * 2 \n",
    "    if row['coastal_dist'] == 0 else row['ef_art'] / row['coastal_dist'], axis=1)\n",
    "\n",
    "df['ef_bch'] = df['beach_length'] / df['coast_length'].replace(0, float('inf'))\n",
    "df['ef_bch'] = df['ef_bch'].replace(float('inf'), 0)\n",
    "df['ef_bch'] = df['ef_bch'] * df['coast_length']\n",
    "df['ef_bch'] = df.apply(\n",
    "    lambda row: row['ef_bch'] * 2 \n",
    "    if row['coastal_dist'] == 0 else row['ef_bch'] / row['coastal_dist'], axis=1)\n",
    "df['ef_river'] = df['river_area'] / df['river_dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06957257",
   "metadata": {},
   "source": [
    "## 2-4 データ整備\n",
    "### 2-4-1列順を変更\n",
    "- コンペ提出時の列順と同じにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('columns_list_sbmt.csv', 'r', newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            col_list = next(reader)\n",
    "df = df.reindex(columns=col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d217ac",
   "metadata": {},
   "source": [
    "### 2-4-2 csvに落としてから読み込む\n",
    "- コンペ提出時の条件と同じにするため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df16139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sbmt_rev1_6_conf.csv')\n",
    "df = pd.read_csv('sbmt_rev1_6_conf.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91aa9f",
   "metadata": {},
   "source": [
    "# 3 学習\n",
    "## 3-1 学習用データ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e60ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['cover'] != 'a']\n",
    "df_train['cover'] = df_train['cover'].astype(float)\n",
    "col_cat = ['mesh20','cluster_id','cls_id_2']\n",
    "for col in col_cat:\n",
    "    df_train[col] = df_train[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e37735",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=['cover'])\n",
    "y_train = df_train[[\"cover\",'cluster_id','cls_id_2']]\n",
    "id_train = pd.DataFrame(df_train.index)\n",
    "id_train['cluster_id'] = df_train['cluster_id']\n",
    "id_train['cls_id_2'] = df_train['cls_id_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b1486",
   "metadata": {},
   "source": [
    "## 3-2 学習\n",
    "### 3-2-1 全データを一括学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bed096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_1(input_X,\n",
    "              input_y,\n",
    "              input_id,\n",
    "              params,\n",
    "              list_nfold=[0,1,2,3,4],\n",
    "              n_splits=5,\n",
    "             ):\n",
    "    train_oof = np.zeros(len(input_X))\n",
    "    metrics = []\n",
    "    imp = pd.DataFrame()\n",
    "    input_y = input_y.drop(columns=['cluster_id','cls_id_2'])\n",
    "    input_id = input_id.drop(columns=['cluster_id','cls_id_2'])\n",
    "                             \n",
    "    cv = list(KFold(n_splits, random_state=123, shuffle=True).split(input_X, input_y))\n",
    "    for nfold in list_nfold:\n",
    "        print(\"-\"*20, nfold, \"-\"*20)\n",
    "        idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "        X_tr, y_tr, id_tr = input_X.loc[idx_tr, :], input_y.loc[idx_tr], input_id.loc[idx_tr, :]\n",
    "        X_va, y_va, id_va = input_X.loc[idx_va, :], input_y.loc[idx_va], input_id.loc[idx_va, :]\n",
    "        print(X_tr.shape, y_tr.shape)\n",
    "        \n",
    "        #train\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(X_tr,\n",
    "                  y_tr,\n",
    "                  eval_set=[(X_tr,y_tr), (X_va,y_va)],\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose=100,\n",
    "                 )\n",
    "        fname_lgb = \"model_sbmt_1_6_lgb_fold{}.pickle\".format(nfold)\n",
    "        with open(fname_lgb, \"wb\") as f:\n",
    "            pickle.dump(model, f, protocol=4)\n",
    "        \n",
    "        #evaluate\n",
    "        y_tr_pred = model.predict(X_tr)\n",
    "        y_va_pred = model.predict(X_va)\n",
    "        metric_tr = np.sqrt(mean_squared_error(y_tr, y_tr_pred))\n",
    "        metric_va = np.sqrt(mean_squared_error(y_va, y_va_pred))\n",
    "        metrics.append([nfold, metric_tr, metric_va])\n",
    "        print(\"[RMSE] tr: {:.4f}, va: {:.4f}\".format(metric_tr, metric_va))\n",
    "        \n",
    "        # oof\n",
    "        train_oof[idx_va] = y_va_pred\n",
    "        \n",
    "        # imp\n",
    "        _imp = pd.DataFrame({\"col\": input_X.columns, \n",
    "                             \"imp\": model.feature_importances_,\"nfold\":nfold})\n",
    "        imp = pd.concat([imp, _imp])\n",
    "    \n",
    "    # metric\n",
    "    print(\"-\"*20, \"result\", \"-\"*20)\n",
    "    metrics = np.array(metrics)\n",
    "    print(metrics)\n",
    "    print(\"[cv] tr: {:.4f}+-{:.4f}, va: {:.4f}+-{:.4f}\".format(\n",
    "        metrics[:,1].mean(), metrics[:,1].std(),\n",
    "        metrics[:,2].mean(), metrics[:,2].std(),\n",
    "    ))\n",
    "    print(\"[oof]{:.4f}\".format(np.sqrt(mean_squared_error(input_y, train_oof))))\n",
    "    \n",
    "    # oof\n",
    "    train_oof = pd.concat([input_id, pd.DataFrame({\"pred\" : train_oof})], axis=1)\n",
    "    \n",
    "    # importance\n",
    "    imp = imp.groupby(\"col\")[\"imp\"].agg([\"mean\", \"std\"]).reset_index(drop=False)\n",
    "    imp.columns = [\"col\", \"imp\", \"imp_std\"]\n",
    "    \n",
    "    return train_oof, imp, metrics     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l2',\n",
    "    'metric': 'Root_Mean_Squared_Error',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 32,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'min_sum_hessian_in_leaf': 50,\n",
    "    'n_estimators': 1000,\n",
    "    'random_state': 123,\n",
    "    'importance_type': 'gain',\n",
    "}\n",
    "\n",
    "train_oof, imp, metrics = train_lgb_1(X_train,\n",
    "                                    y_train,\n",
    "                                    id_train,\n",
    "                                    params,\n",
    "                                    list_nfold=[0,1,2,3,4],\n",
    "                                    n_splits=5,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4242c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.sort_values('imp',ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277b20b",
   "metadata": {},
   "source": [
    "### 3-2-2 地理的な３つの区域毎に学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d8d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_3(input_X,\n",
    "              input_y,\n",
    "              input_id,\n",
    "              params,\n",
    "              list_nfold=[0, 1, 2, 3, 4],\n",
    "              n_splits=5,\n",
    "              ):\n",
    "    metrics_list = []\n",
    "    train_oof_list =[]\n",
    "    imp_list =[]\n",
    "    for i in range(3):\n",
    "            tmp_X = input_X[input_X['cluster_id'] == i]\n",
    "            tmp_y = input_y[input_y['cluster_id'] == i]\n",
    "            tmp_id = input_id[input_id['cluster_id'] == i]\n",
    "\n",
    "            tmp_y.drop(columns=['cluster_id','cls_id_2'], inplace=True)\n",
    "            tmp_id.drop(columns=['cluster_id','cls_id_2'], inplace=True)\n",
    "            tmp_X.reset_index(drop=True, inplace=True)\n",
    "            tmp_y.reset_index(drop=True, inplace=True)\n",
    "            tmp_id.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            train_oof = np.zeros(len(tmp_X))\n",
    "            metrics = []\n",
    "            imp = pd.DataFrame()\n",
    "\n",
    "                # cross-validation\n",
    "            cv = list(KFold(n_splits, random_state=123, shuffle=True).split(\n",
    "                    tmp_X, tmp_y))\n",
    "\n",
    "            for nfold in list_nfold:\n",
    "                print(\"-\" * 20, nfold, \"-\" * 20)\n",
    "                idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "                X_tr, y_tr, id_tr = tmp_X.loc[idx_tr, :], tmp_y.loc[idx_tr], \\\n",
    "                                    tmp_id.loc[idx_tr, :]\n",
    "                X_va, y_va, id_va = tmp_X.loc[idx_va, :], tmp_y.loc[idx_va], \\\n",
    "                                    tmp_id.loc[idx_va, :]\n",
    "                print(X_tr.shape, y_tr.shape)\n",
    "\n",
    "                # train\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_tr,\n",
    "                          y_tr,\n",
    "                          eval_set=[(X_tr, y_tr), (X_va, y_va)],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose=100,\n",
    "                         )\n",
    "                fname_lgb = \"model_sbmt_1_6_cls{0}_lgb_fold{1}.pickle\".format(i, nfold)\n",
    "                with open(fname_lgb, \"wb\") as f:\n",
    "                    pickle.dump(model, f, protocol=4)\n",
    "                    \n",
    "                # evaluate\n",
    "                y_tr_pred = model.predict(X_tr)\n",
    "                y_va_pred = model.predict(X_va)\n",
    "                metric_tr = np.sqrt(mean_squared_error(y_tr, y_tr_pred))\n",
    "                metric_va = np.sqrt(mean_squared_error(y_va, y_va_pred))\n",
    "                metrics.append([nfold, metric_tr, metric_va])\n",
    "                print(\"[RMSE] tr: {:.4f}, va: {:.4f}\".format(metric_tr, metric_va))\n",
    "\n",
    "                # oof\n",
    "                train_oof[idx_va] = y_va_pred\n",
    "\n",
    "                # imp\n",
    "                _imp = pd.DataFrame(\n",
    "                    {\"col\": tmp_X.columns, \"imp\": model.feature_importances_, \n",
    "                     \"nfold\": nfold})\n",
    "                imp = pd.concat([imp, _imp])\n",
    "\n",
    "            # metric\n",
    "            print(\"-\" * 20, \"cls{}_result\".format(i), \"-\" * 20)\n",
    "            metrics = np.array(metrics)\n",
    "            print(metrics)\n",
    "            print(\"[cv] tr: {:.4f}+-{:.4f}, va: {:.4f}+-{:.4f}\".format(\n",
    "                metrics[:, 1].mean(), metrics[:, 1].std(),\n",
    "                metrics[:, 2].mean(), metrics[:, 2].std(),\n",
    "                ))\n",
    "            print(\"[oof]{:.4f}\".format(np.sqrt(mean_squared_error(tmp_y, train_oof))))\n",
    "            metrics_list.append(metrics)\n",
    "\n",
    "            # oof\n",
    "            train_oof = pd.concat([tmp_id, pd.DataFrame({\"pred\": train_oof})], axis=1)\n",
    "            train_oof_list.append(train_oof)\n",
    "\n",
    "            # importance\n",
    "            imp = imp.groupby(\"col\")[\"imp\"].agg([\"mean\", \"std\"]).reset_index(drop=False)\n",
    "            imp.columns = [\"col\", \"imp\", \"imp_std\"]\n",
    "            imp_list.append(imp)\n",
    "\n",
    "    return train_oof_list, imp_list, metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l2',\n",
    "    'metric': 'Root_Mean_Squared_Error',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 32,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'min_sum_hessian_in_leaf': 50,\n",
    "    'n_estimators': 1000,\n",
    "    'random_state': 123,\n",
    "    'importance_type': 'gain',\n",
    "}\n",
    "\n",
    "train_oof, imp, metrics = train_lgb_3(X_train,\n",
    "                                    y_train,\n",
    "                                    id_train,\n",
    "                                    params,\n",
    "                                    list_nfold=[0,1,2,3,4],\n",
    "                                    n_splits=5,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp[2].sort_values('imp',ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d209bcc",
   "metadata": {},
   "source": [
    "### 3-2-3 地理的区域毎、かつサブクラス毎に学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_9(input_X,\n",
    "              input_y,\n",
    "              input_id,\n",
    "              params,\n",
    "              list_nfold=[0, 1, 2, 3, 4],\n",
    "              n_splits=5,\n",
    "              cls_num=3,\n",
    "              cls_num_2=3,\n",
    "              ):\n",
    "    metrics_list = []\n",
    "    train_oof_list =[]\n",
    "    imp_list =[]\n",
    "    for i in range(cls_num):\n",
    "        tmp_2_X = input_X[input_X['cluster_id'] == i]\n",
    "        tmp_2_y = input_y[input_y['cluster_id'] == i]\n",
    "        tmp_2_id = input_id[input_id['cluster_id'] == i]\n",
    "        \n",
    "        for j in range(cls_num_2):\n",
    "                tmp_X = tmp_2_X[tmp_2_X['cls_id_2'] == j]\n",
    "                tmp_y = tmp_2_y[tmp_2_y['cls_id_2'] == j]\n",
    "                tmp_id = tmp_2_id[tmp_2_id['cls_id_2'] == j]\n",
    "            \n",
    "                tmp_y.drop(columns=['cluster_id','cls_id_2'], inplace=True)\n",
    "                tmp_id.drop(columns=['cluster_id','cls_id_2'], inplace=True)\n",
    "                tmp_X.reset_index(drop=True, inplace=True)\n",
    "                tmp_y.reset_index(drop=True, inplace=True)\n",
    "                tmp_id.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                train_oof = np.zeros(len(tmp_X))\n",
    "                metrics = []\n",
    "                imp = pd.DataFrame()\n",
    "\n",
    "                # cross-validation\n",
    "                cv = list(KFold(n_splits, random_state=123, shuffle=True).split(\n",
    "                    tmp_X, tmp_y))\n",
    "\n",
    "                for nfold in list_nfold:\n",
    "                    print(\"-\" * 20, nfold, \"-\" * 20)\n",
    "                    idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "                    X_tr, y_tr, id_tr = tmp_X.loc[idx_tr, :], tmp_y.loc[idx_tr], \\\n",
    "                                        tmp_id.loc[idx_tr, :]\n",
    "                    X_va, y_va, id_va = tmp_X.loc[idx_va, :], tmp_y.loc[idx_va], \\\n",
    "                                        tmp_id.loc[idx_va, :]\n",
    "                    print(X_tr.shape, y_tr.shape)\n",
    "\n",
    "                    # train\n",
    "                    model = lgb.LGBMRegressor(**params)\n",
    "                    model.fit(X_tr,\n",
    "                              y_tr,\n",
    "                              eval_set=[(X_tr, y_tr), (X_va, y_va)],\n",
    "                              early_stopping_rounds=50,\n",
    "                              verbose=100,\n",
    "                              )\n",
    "                    fname_lgb = \"model_sbmt_1_6_cls{0}_{1}_lgb_fold{2}.pickle\".format(\n",
    "                        i,j, nfold)\n",
    "                    with open(fname_lgb, \"wb\") as f:\n",
    "                        pickle.dump(model, f, protocol=4)\n",
    "\n",
    "                    # evaluate\n",
    "                    y_tr_pred = model.predict(X_tr)\n",
    "                    y_va_pred = model.predict(X_va)\n",
    "                    metric_tr = np.sqrt(mean_squared_error(y_tr, y_tr_pred))\n",
    "                    metric_va = np.sqrt(mean_squared_error(y_va, y_va_pred))\n",
    "                    metrics.append([nfold, metric_tr, metric_va])\n",
    "                    print(\"[RMSE] tr: {:.4f}, va: {:.4f}\".format(metric_tr, metric_va))\n",
    "\n",
    "                    # oof\n",
    "                    train_oof[idx_va] = y_va_pred\n",
    "\n",
    "                    # imp\n",
    "                    _imp = pd.DataFrame(\n",
    "                        {\"col\": tmp_X.columns, \n",
    "                         \"imp\": model.feature_importances_, \n",
    "                         \"nfold\": nfold})\n",
    "                    imp = pd.concat([imp, _imp])\n",
    "\n",
    "                # metric\n",
    "                print(\"-\" * 20, \"cls{}_{}_result\".format(i,j), \"-\" * 20)\n",
    "                print(fname_lgb)\n",
    "                metrics = np.array(metrics)\n",
    "                print(metrics)\n",
    "                print(\"[cv] tr: {:.4f}+-{:.4f}, va: {:.4f}+-{:.4f}\".format(\n",
    "                    metrics[:, 1].mean(), metrics[:, 1].std(),\n",
    "                    metrics[:, 2].mean(), metrics[:, 2].std(),\n",
    "                ))\n",
    "                print(\"[oof]{:.4f}\".format(np.sqrt(mean_squared_error(tmp_y, train_oof))))\n",
    "                metrics_list.append(metrics)\n",
    "\n",
    "                # oof\n",
    "                train_oof = pd.concat([tmp_id, pd.DataFrame({\"pred\": train_oof})], axis=1)\n",
    "                train_oof_list.append(train_oof)\n",
    "\n",
    "                # importance\n",
    "                imp = imp.groupby(\"col\")[\"imp\"].agg([\"mean\", \"std\"]).reset_index(drop=False)\n",
    "                imp.columns = [\"col\", \"imp\", \"imp_std\"]\n",
    "                imp_list.append(imp)        \n",
    "\n",
    "    return train_oof_list, imp_list, metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l2',\n",
    "    'metric': 'Root_Mean_Squared_Error',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 32,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'min_sum_hessian_in_leaf': 50,\n",
    "    'n_estimators': 1000,\n",
    "    'random_state': 123,\n",
    "    'importance_type': 'gain',\n",
    "}\n",
    "\n",
    "train_oof_list, imp_list, metrics_list = train_lgb_9(X_train,\n",
    "                                                   y_train,\n",
    "                                                   id_train,\n",
    "                                                   params,\n",
    "                                                   list_nfold=[0,1,2,3,4],\n",
    "                                                   n_splits=5,\n",
    "                                                   cls_num=3,\n",
    "                                                   cls_num_2=3,\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_list[8].sort_values('imp',ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f7b98",
   "metadata": {},
   "source": [
    "# 4 予測\n",
    "## 4-1 予測用データ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d368928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df['cover'] == 'a'].reset_index(drop=True)\n",
    "df_test = df_test.drop(columns=['cover'])\n",
    "col_cat = ['mesh20','cluster_id','cls_id_2']\n",
    "for col in col_cat:\n",
    "    df_test[col] = df_test[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.copy()\n",
    "id_test = pd.DataFrame(df_test.index)\n",
    "id_test['cluster_id'] = df_test['cluster_id']\n",
    "id_test['cls_id_2'] = df_test['cls_id_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec2844",
   "metadata": {},
   "source": [
    "## 4-2 予測\n",
    "- ３つの学習モデルをアンサンブル実施。重みづけは実施せず、平均値を予測値とした"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067487e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lgb_9(input_X,\n",
    "                input_id,\n",
    "                list_nfold_0=[0,1,2,3,4],\n",
    "                list_nfold_1=[0,1,2,3,4],\n",
    "                list_nfold_2=[0,1,2,3,4],\n",
    "               ):\n",
    "    pred_list = []\n",
    "    for i in range(3):\n",
    "            tmp_2_X = input_X[input_X['cluster_id'] == i]\n",
    "            tmp_2_id = input_id[input_id['cluster_id'] == i]\n",
    "            tmp_2_id.drop(columns=['cluster_id'], inplace=True)\n",
    "            \n",
    "            for j in range(3):                 \n",
    "                    tmp_X = tmp_2_X[tmp_2_X['cls_id_2'] == j]\n",
    "                    tmp_id = tmp_2_id[tmp_2_id['cls_id_2'] == j]\n",
    "                    tmp_id.drop(columns=['cls_id_2'], inplace=True)\n",
    "                    pred = np.zeros((len(tmp_X), \n",
    "                                     len(list_nfold_0)\n",
    "                                    + len(list_nfold_1) \n",
    "                                     + len(list_nfold_2)))\n",
    "                    \n",
    "                    for nfold in list_nfold_2:\n",
    "                        print(\"-\"*20, nfold, \"-\"*20)\n",
    "                        fname_lgb_1 = \"model_sbmt_1_6_cls{}_lgb_fold{}.pickle\".format(\n",
    "                            i,nfold)\n",
    "                        with open(fname_lgb_1, \"rb\") as f:\n",
    "                            model = pickle.load(f)\n",
    "                        pred[:, len(list_nfold_0) + \n",
    "                             len(list_nfold_1) + nfold] = model.predict(tmp_X)\n",
    "                                            \n",
    "                    for nfold in list_nfold_1:\n",
    "                        print(\"-\"*20, nfold, \"-\"*20)\n",
    "                        fname_lgb_2 = \"model_sbmt_1_6_cls{}_{}_lgb_fold{}.pickle\".format(\n",
    "                            i,j,nfold)\n",
    "                        with open(fname_lgb_2, \"rb\") as f:\n",
    "                            model = pickle.load(f)\n",
    "                        pred[:, len(list_nfold_0) + nfold] = model.predict(tmp_X)\n",
    "                    \n",
    "                    for nfold in list_nfold_0:\n",
    "                        print(\"-\"*20, nfold, \"-\"*20)\n",
    "                        fname_lgb_3 = \"model_sbmt_1_6_lgb_fold{}.pickle\".format(nfold)\n",
    "                        with open(fname_lgb_3, \"rb\") as f:\n",
    "                            model = pickle.load(f)\n",
    "                        pred[:, nfold] = model.predict(tmp_X)\n",
    "                    \n",
    "                    df_pred = pd.DataFrame({\"pred\": pred.mean(axis=1)})\n",
    "                    df_pred.index = tmp_id.index\n",
    "                    pred = pd.concat([\n",
    "                        tmp_id,df_pred,], axis=1)\n",
    "                    pred_list.append(pred)\n",
    "                    text = 'cls{}_{}_Done.'.format(i,j)\n",
    "                    print(len(tmp_X))\n",
    "                    print(text + '_' + fname_lgb_3 + '_' + fname_lgb_1 + '_' + fname_lgb_2)\n",
    "                    print(len(pred))                    \n",
    "\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_2 = predict_lgb_9(\n",
    "    X_test,id_test,list_nfold_1=[0,1,2,3,4],list_nfold_2=[0,1,2,3,4],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65072c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.DataFrame()\n",
    "for i in range(9):\n",
    "    tmp = test_pred_2[i]\n",
    "    test_pred = pd.concat([test_pred,tmp])\n",
    "test_pred = test_pred.sort_values(0)\n",
    "# 負の値はゼロに、１を超えた値は１に修正\n",
    "test_pred['pred'] = test_pred['pred'].apply(lambda x: 0 if x<0 else (1 if x>1 else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.to_csv(\"sbmt_test_1_6.csv\", header=False,index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
